{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "from torchvision.models import inception_v3\n",
    "import torch.nn.functional as F\n",
    "\n",
    "size = (1, 224, 224)\n",
    "\n",
    "class ImageColorNet(nn.Module):\n",
    "    def __init__(self, batch_size):\n",
    "        super(ImageColorNet, self).__init__()\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # encoder - input\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=(2, 2), stride=2)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(32, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv4 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # add layers\n",
    "        \n",
    "        self.conv5 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool5 = nn.MaxPool2d(kernel_size=(2, 2), stride=2)\n",
    "\n",
    "        self.conv6 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv7 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv8 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.conv9 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool9 = nn.MaxPool2d(kernel_size=(2, 2), stride=2)\n",
    "\n",
    "        self.conv_10 = nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv_11 = nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.conv_12 = nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv_13 = nn.Conv2d(512, 256, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.model_inceptionv3 = inception_v3(pretrained=True, aux_logits=False)\n",
    "        self.linear1 = nn.Linear(2048, 128)\n",
    "        self.linear2 = nn.Linear(128, 1)\n",
    "\n",
    "        self.layernorm = nn.LayerNorm([28, 28])\n",
    "\n",
    "        for i, param in self.model_inceptionv3.named_parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        num_ftrs = self.model_inceptionv3.fc.in_features\n",
    "        self.model_inceptionv3.fc = nn.Linear(num_ftrs, num_ftrs*128)\n",
    "\n",
    "        for name, child in self.model_inceptionv3.named_children():\n",
    "            for params in child.parameters():\n",
    "                params.requires_grad = False\n",
    "                \n",
    "        self.conv_15 = nn.Conv2d(384, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv_16 = nn.ConvTranspose2d(128, 64, kernel_size=5, stride=2, padding=1)\n",
    "        self.conv_17 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv_18 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv_19 = nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv_20 = nn.Conv2d(32, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv_21 = nn.Conv2d(16, 2, kernel_size=3, stride=1, padding=(1, 1))\n",
    "        self.conv_22 = nn.ConvTranspose2d(2, 2, kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "    def forward(self, output, output_inception):\n",
    "        output = output.view(self.batch_size,1,225,225)\n",
    "        \n",
    "        x = F.relu(self.conv1(output))\n",
    "        x = self.pool1(x)\n",
    "        conv2 = F.relu(self.conv2(x))\n",
    "        conv3 = F.relu(self.conv3(conv2))\n",
    "        conv4 = F.relu(self.conv4(conv3))\n",
    "        \n",
    "        layer4 = conv2 + conv4\n",
    "        \n",
    "        x = F.relu(self.conv5(layer4))\n",
    "        x = self.pool5(x)\n",
    "        conv6 = F.relu(self.conv6(x))\n",
    "        conv7 = F.relu(self.conv7(conv6))\n",
    "        conv8 = F.relu(self.conv8(conv7))\n",
    "        \n",
    "        layer8 = conv6 + conv8\n",
    "        \n",
    "        x = F.relu(self.conv9(layer8))\n",
    "        x = self.pool9(x)\n",
    "        conv_10 = F.relu(self.conv_10(x))\n",
    "        conv_11 = F.relu(self.conv_11(conv_10))\n",
    "        conv_12 = F.relu(self.conv_12(conv_11))\n",
    "        \n",
    "        layer_12 = conv_10 + conv_12\n",
    "        \n",
    "        x = F.relu(self.conv_13(layer_12))\n",
    "        \n",
    "        inception_output = self.model_inceptionv3.forward(output_inception.view(self.batch_size,3,299,299))\n",
    "        inception_output = self.linear1(inception_output.view(self.batch_size, 128, 2048))\n",
    "        inception_output = self.linear2(inception_output.view(self.batch_size, 128, 128))\n",
    "        \n",
    "        inception_output = inception_output.view(self.batch_size, 128)\n",
    "        inception_output = inception_output.repeat(28, 28)\n",
    "        inception_output = inception_output.view(self.batch_size, 128, 28, 28)\n",
    "        inception_output = self.layernorm(inception_output)\n",
    "        \n",
    "        x = torch.cat([inception_output, x], dim=1)\n",
    "        \n",
    "        x = self.conv_15(x)\n",
    "        x = F.relu(self.conv_16(x))\n",
    "        x = F.relu(self.conv_17(x))\n",
    "        x = F.relu(self.conv_18(x))\n",
    "        x = F.relu(self.conv_19(x))\n",
    "        x = F.relu(self.conv_20(x))\n",
    "        x = F.relu(self.conv_21(x))\n",
    "        x = F.relu(self.conv_22(x))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------\n",
      "         Layer (type)            Output Shape         Param #     Tr. Param #\n",
      "==============================================================================\n",
      "             Conv2d-1      [32, 32, 225, 225]             320             320\n",
      "          MaxPool2d-2      [32, 32, 112, 112]               0               0\n",
      "             Conv2d-3     [32, 128, 112, 112]          36,992          36,992\n",
      "             Conv2d-4     [32, 128, 112, 112]         147,584         147,584\n",
      "             Conv2d-5     [32, 128, 112, 112]         147,584         147,584\n",
      "             Conv2d-6     [32, 128, 112, 112]         147,584         147,584\n",
      "          MaxPool2d-7       [32, 128, 56, 56]               0               0\n",
      "             Conv2d-8       [32, 256, 56, 56]         295,168         295,168\n",
      "             Conv2d-9       [32, 256, 56, 56]         590,080         590,080\n",
      "            Conv2d-10       [32, 256, 56, 56]         590,080         590,080\n",
      "            Conv2d-11       [32, 256, 56, 56]         590,080         590,080\n",
      "         MaxPool2d-12       [32, 256, 28, 28]               0               0\n",
      "            Conv2d-13       [32, 512, 28, 28]       1,180,160       1,180,160\n",
      "            Conv2d-14       [32, 512, 28, 28]       2,359,808       2,359,808\n",
      "            Conv2d-15       [32, 512, 28, 28]       2,359,808       2,359,808\n",
      "            Conv2d-16       [32, 256, 28, 28]       1,179,904       1,179,904\n",
      "            Linear-17               [32, 128]         262,272         262,272\n",
      "            Conv2d-18       [32, 128, 28, 28]         442,496         442,496\n",
      "   ConvTranspose2d-19        [32, 64, 57, 57]         204,864         204,864\n",
      "            Conv2d-20        [32, 64, 57, 57]          36,928          36,928\n",
      "            Conv2d-21        [32, 64, 57, 57]          36,928          36,928\n",
      "   ConvTranspose2d-22      [32, 32, 113, 113]          18,464          18,464\n",
      "            Conv2d-23      [32, 16, 113, 113]           4,624           4,624\n",
      "            Conv2d-24       [32, 2, 113, 113]             290             290\n",
      "   ConvTranspose2d-25       [32, 2, 225, 225]              38              38\n",
      "==============================================================================\n",
      "Total params: 10,632,056\n",
      "Trainable params: 10,632,056\n",
      "Non-trainable params: 0\n",
      "------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from pytorch_model_summary import summary\n",
    "\n",
    "print(summary(ImageColorNet(32), torch.zeros(32,1,225,225), torch.zeros(32,3,299,299), show_input=False, show_hierarchical=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pytorch_colors as colors\n",
    "\n",
    "batch_size = 32\n",
    "net = ImageColorNet(32)\n",
    "net.eval()\n",
    "\n",
    "transform = transforms.Compose([transforms.Resize((225,225)),\n",
    "                                transforms.ToTensor()])\n",
    "\n",
    "inception_transform = transforms.Compose([transforms.Resize((299,299)),\n",
    "                                transforms.Grayscale(num_output_channels=3),\n",
    "                                transforms.ToTensor()])\n",
    "\n",
    "dataset = datasets.ImageFolder(\"E:\\\\Drives-Linux-ubuntu-2020\\\\home\\\\aswin\\\\Documents\\\\Deep-Learning-Projects\\\\open_images_dataset\\\\dataset\\\\test\", transform=transform)\n",
    "inception_dataset = datasets.ImageFolder(\"E:\\\\Drives-Linux-ubuntu-2020\\\\home\\\\aswin\\\\Documents\\\\Deep-Learning-Projects\\\\open_images_dataset\\\\dataset\\\\test\", transform=inception_transform)\n",
    "\n",
    "dataloader = enumerate(torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False))\n",
    "inception_dataloader = enumerate(torch.utils.data.DataLoader(inception_dataset, batch_size=batch_size, shuffle=False))\n",
    "i, (images, labels) = next(iter(dataloader))\n",
    "lab_images = rgb2lab(images)\n",
    "i, (inception_images, inception_labels) = next(iter(inception_dataloader))\n",
    "checkpoint = torch.load('checkpoints/img_color-1_170.checkpoint.pth')\n",
    "with torch.no_grad():\n",
    "    net.load_state_dict(checkpoint['model_state_dict'])\n",
    "    o = net.forward(lab_images[:,0,:,:] / 255, inception_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_images = torch.cat([lab_images[:,0,:,:].unsqueeze(1), o], dim=1)\n",
    "rgb_images = lab2rgb(new_images)*255\n",
    "import cv2\n",
    "cv2.imwrite(\"image.png\", rgb_images[0].permute(1,2,0).int().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.imwrite(\"image.png\", rgb_images[3].permute(1,2,0).int().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/richzhang/colorization-pytorch/blob/master/util/util.py\n",
    "\n",
    "import torch\n",
    "\n",
    "# Color conversion code\n",
    "def rgb2xyz(rgb): # rgb from [0,1]\n",
    "\n",
    "    mask = (rgb > .04045).type(torch.FloatTensor)\n",
    "    if(rgb.is_cuda):\n",
    "        mask = mask.cuda()\n",
    "\n",
    "    rgb = (((rgb+.055)/1.055)**2.4)*mask + rgb/12.92*(1-mask)\n",
    "\n",
    "    x = .412453*rgb[:,0,:,:]+.357580*rgb[:,1,:,:]+.180423*rgb[:,2,:,:]\n",
    "    y = .212671*rgb[:,0,:,:]+.715160*rgb[:,1,:,:]+.072169*rgb[:,2,:,:]\n",
    "    z = .019334*rgb[:,0,:,:]+.119193*rgb[:,1,:,:]+.950227*rgb[:,2,:,:]\n",
    "    out = torch.cat((x[:,None,:,:],y[:,None,:,:],z[:,None,:,:]),dim=1)\n",
    "\n",
    "    return out\n",
    "\n",
    "def xyz2rgb(xyz):\n",
    "\n",
    "    r = 3.24048134*xyz[:,0,:,:]-1.53715152*xyz[:,1,:,:]-0.49853633*xyz[:,2,:,:]\n",
    "    g = -0.96925495*xyz[:,0,:,:]+1.87599*xyz[:,1,:,:]+.04155593*xyz[:,2,:,:]\n",
    "    b = .05564664*xyz[:,0,:,:]-.20404134*xyz[:,1,:,:]+1.05731107*xyz[:,2,:,:]\n",
    "\n",
    "    rgb = torch.cat((r[:,None,:,:],g[:,None,:,:],b[:,None,:,:]),dim=1)\n",
    "    rgb = torch.max(rgb,torch.zeros_like(rgb)) # sometimes reaches a small negative number, which causes NaNs\n",
    "\n",
    "    mask = (rgb > .0031308).type(torch.FloatTensor)\n",
    "    if(rgb.is_cuda):\n",
    "        mask = mask.cuda()\n",
    "\n",
    "    rgb = (1.055*(rgb**(1./2.4)) - 0.055)*mask + 12.92*rgb*(1-mask)\n",
    "\n",
    "    return rgb\n",
    "\n",
    "def xyz2lab(xyz):\n",
    "    # 0.95047, 1., 1.08883 # white\n",
    "    sc = torch.Tensor((0.95047, 1., 1.08883))[None,:,None,None]\n",
    "    if(xyz.is_cuda):\n",
    "        sc = sc.cuda()\n",
    "\n",
    "    xyz_scale = xyz/sc\n",
    "\n",
    "    mask = (xyz_scale > .008856).type(torch.FloatTensor)\n",
    "    if(xyz_scale.is_cuda):\n",
    "        mask = mask.cuda()\n",
    "\n",
    "    xyz_int = xyz_scale**(1/3.)*mask + (7.787*xyz_scale + 16./116.)*(1-mask)\n",
    "\n",
    "    L = 116.*xyz_int[:,1,:,:]-16.\n",
    "    a = 500.*(xyz_int[:,0,:,:]-xyz_int[:,1,:,:])\n",
    "    b = 200.*(xyz_int[:,1,:,:]-xyz_int[:,2,:,:])\n",
    "    out = torch.cat((L[:,None,:,:],a[:,None,:,:],b[:,None,:,:]),dim=1)\n",
    "\n",
    "    return out\n",
    "\n",
    "def lab2xyz(lab):\n",
    "    y_int = (lab[:,0,:,:]+16.)/116.\n",
    "    x_int = (lab[:,1,:,:]/500.) + y_int\n",
    "    z_int = y_int - (lab[:,2,:,:]/200.)\n",
    "    if(z_int.is_cuda):\n",
    "        z_int = torch.max(torch.Tensor((0,)).cuda(), z_int)\n",
    "    else:\n",
    "        z_int = torch.max(torch.Tensor((0,)), z_int)\n",
    "\n",
    "    out = torch.cat((x_int[:,None,:,:],y_int[:,None,:,:],z_int[:,None,:,:]),dim=1)\n",
    "    mask = (out > .2068966).type(torch.FloatTensor)\n",
    "    if(out.is_cuda):\n",
    "        mask = mask.cuda()\n",
    "\n",
    "    out = (out**3.)*mask + (out - 16./116.)/7.787*(1-mask)\n",
    "\n",
    "    sc = torch.Tensor((0.95047, 1., 1.08883))[None,:,None,None]\n",
    "    sc = sc.to(out.device)\n",
    "\n",
    "    out = out*sc\n",
    "\n",
    "    return out\n",
    "\n",
    "def rgb2lab(rgb):\n",
    "    lab = xyz2lab(rgb2xyz(rgb))\n",
    "    l_rs = (lab[:,[0],:,:])/100\n",
    "    ab_rs = (lab[:,1:,:,:]+128)/255\n",
    "    out = torch.cat((l_rs,ab_rs),dim=1)\n",
    "    return out\n",
    "\n",
    "def lab2rgb(lab_rs):\n",
    "    l = lab_rs[:,[0],:,:]*100\n",
    "    ab = lab_rs[:,1:,:,:]*255 - 128\n",
    "    lab = torch.cat((l,ab),dim=1)\n",
    "    out = xyz2rgb(lab2xyz(lab))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 1.2142e-01,  1.2142e-01,  1.2142e-01,  ...,  2.3181e-01,\n",
       "            2.5050e-01,  2.2902e-01],\n",
       "          [ 1.2142e-01,  1.2142e-01,  1.2142e-01,  ...,  2.0750e-01,\n",
       "            2.3300e-01,  2.2099e-01],\n",
       "          [ 1.2142e-01,  1.2142e-01,  1.2142e-01,  ...,  1.8631e-01,\n",
       "            2.0323e-01,  2.2191e-01],\n",
       "          ...,\n",
       "          [ 9.0152e-02,  1.0846e-01,  9.0712e-02,  ...,  2.5054e-01,\n",
       "            2.7492e-01,  2.8065e-01],\n",
       "          [ 8.0519e-02,  9.6463e-02,  8.8521e-02,  ...,  2.7767e-01,\n",
       "            2.9780e-01,  3.0633e-01],\n",
       "          [ 7.9336e-02,  7.5061e-02,  7.5681e-02,  ...,  2.9588e-01,\n",
       "            3.0299e-01,  3.0178e-01]],\n",
       "\n",
       "         [[-3.4652e-02, -3.4652e-02, -3.4652e-02,  ...,  2.2017e-01,\n",
       "            1.5568e-01,  1.0370e-01],\n",
       "          [-3.4652e-02, -3.4652e-02, -3.4652e-02,  ...,  2.1241e-01,\n",
       "            2.1569e-01,  1.9296e-01],\n",
       "          [-3.4652e-02, -3.4652e-02, -3.4652e-02,  ...,  1.1260e-01,\n",
       "            1.9418e-01,  2.0991e-01],\n",
       "          ...,\n",
       "          [ 3.9505e-02,  1.8731e-02,  3.4397e-02,  ...,  1.7633e-01,\n",
       "            1.8044e-01,  1.8015e-01],\n",
       "          [ 4.1084e-02,  2.5275e-02,  3.3308e-02,  ...,  1.5796e-01,\n",
       "            1.6657e-01,  1.5908e-01],\n",
       "          [ 5.7089e-02,  5.0360e-02,  4.4780e-02,  ...,  1.5788e-01,\n",
       "            1.5792e-01,  1.5160e-01]]],\n",
       "\n",
       "\n",
       "        [[[-4.8589e-02, -4.1955e-02, -8.0790e-02,  ..., -2.4838e-02,\n",
       "           -5.2481e-02, -5.7900e-02],\n",
       "          [-4.6514e-02, -3.9949e-02, -5.9589e-02,  ..., -3.8274e-02,\n",
       "           -5.6238e-02, -6.7192e-02],\n",
       "          [-6.5681e-02, -4.2814e-02, -5.5826e-02,  ..., -4.1886e-02,\n",
       "           -6.4500e-02, -6.7192e-02],\n",
       "          ...,\n",
       "          [ 1.3276e-01,  1.5122e-01,  1.6589e-01,  ..., -6.9512e-02,\n",
       "           -5.6974e-02, -5.9265e-02],\n",
       "          [ 1.2022e-01,  1.4899e-01,  1.7692e-01,  ..., -6.4195e-02,\n",
       "           -5.7027e-02, -4.8807e-02],\n",
       "          [ 1.2810e-01,  1.5524e-01,  1.8773e-01,  ..., -5.4726e-02,\n",
       "           -6.0130e-02, -5.0842e-02]],\n",
       "\n",
       "         [[ 3.1695e-02,  2.3325e-02,  8.8283e-02,  ...,  9.6980e-02,\n",
       "            1.2171e-01,  1.8993e-01],\n",
       "          [ 2.6719e-02,  1.8419e-02,  5.0458e-02,  ...,  1.1471e-01,\n",
       "            1.2073e-01,  1.7531e-01],\n",
       "          [ 5.8391e-02,  1.7158e-02,  4.0539e-02,  ...,  1.4133e-01,\n",
       "            1.3487e-01,  1.7531e-01],\n",
       "          ...,\n",
       "          [ 1.6275e-01,  1.5530e-01,  1.4692e-01,  ...,  2.0662e-01,\n",
       "            2.1061e-01,  2.2004e-01],\n",
       "          [ 1.5735e-01,  1.4883e-01,  1.4824e-01,  ...,  2.0307e-01,\n",
       "            2.1027e-01,  2.1207e-01],\n",
       "          [ 1.4986e-01,  1.5328e-01,  1.4992e-01,  ...,  2.0041e-01,\n",
       "            2.0328e-01,  1.9966e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 1.1572e-02,  1.6544e-02,  1.2406e-02,  ..., -4.8820e-02,\n",
       "           -5.7207e-02, -5.7703e-02],\n",
       "          [ 9.1266e-03,  9.4813e-03,  1.9844e-02,  ..., -4.7583e-02,\n",
       "           -5.9838e-02, -6.0991e-02],\n",
       "          [ 1.1572e-02,  1.1919e-02,  1.4991e-02,  ..., -4.2718e-02,\n",
       "           -5.7207e-02, -5.4676e-02],\n",
       "          ...,\n",
       "          [ 9.5966e-03,  1.4858e-03,  4.5402e-03,  ..., -1.9036e-03,\n",
       "           -9.7042e-03, -2.8114e-03],\n",
       "          [-3.8499e-04,  3.6305e-04,  1.4741e-03,  ...,  5.2588e-04,\n",
       "           -3.4785e-03, -1.6635e-03],\n",
       "          [-1.5289e-03, -4.8323e-03, -7.7540e-04,  ..., -9.3997e-03,\n",
       "           -1.0525e-02, -1.0388e-02]],\n",
       "\n",
       "         [[-6.4133e-02, -6.9937e-02, -7.6216e-02,  ...,  4.5470e-03,\n",
       "           -3.9865e-03, -1.1282e-02],\n",
       "          [-5.7492e-02, -6.7464e-02, -7.8841e-02,  ...,  5.7375e-03,\n",
       "            2.2106e-03, -1.2283e-02],\n",
       "          [-6.4133e-02, -6.6601e-02, -7.5295e-02,  ..., -6.9919e-03,\n",
       "           -3.9865e-03, -1.7510e-02],\n",
       "          ...,\n",
       "          [-7.1255e-02, -6.8650e-02, -6.7404e-02,  ..., -1.1704e-01,\n",
       "           -1.1487e-01, -1.0770e-01],\n",
       "          [-6.4274e-02, -7.4559e-02, -6.8576e-02,  ..., -1.3742e-01,\n",
       "           -1.2879e-01, -1.1801e-01],\n",
       "          [-7.0205e-02, -8.7209e-02, -8.0132e-02,  ..., -1.3019e-01,\n",
       "           -1.2062e-01, -1.2124e-01]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[-3.5111e-02,  4.1259e-02,  2.0236e-02,  ..., -9.3010e-03,\n",
       "           -6.5058e-03, -6.5058e-03],\n",
       "          [ 5.8183e-02,  1.4901e-02, -4.5075e-02,  ..., -4.9464e-03,\n",
       "           -4.9464e-03, -4.9464e-03],\n",
       "          [ 5.7231e-03, -1.7012e-02,  3.7360e-02,  ..., -1.0432e-02,\n",
       "           -1.0432e-02, -5.4928e-03],\n",
       "          ...,\n",
       "          [ 1.7130e-02,  2.3074e-02,  2.0688e-02,  ...,  2.3462e-02,\n",
       "            3.0727e-02,  3.2470e-02],\n",
       "          [ 1.8499e-02,  3.0969e-02,  3.6027e-02,  ...,  2.5249e-02,\n",
       "            2.5249e-02,  2.5204e-02],\n",
       "          [ 1.8011e-02,  2.5337e-02,  3.6160e-02,  ...,  2.5295e-02,\n",
       "            2.5295e-02,  2.5342e-02]],\n",
       "\n",
       "         [[ 3.4345e-02, -7.0042e-02, -6.2066e-02,  ..., -3.3092e-01,\n",
       "           -3.3548e-01, -3.3548e-01],\n",
       "          [-9.1898e-02, -5.8446e-02, -9.1724e-03,  ..., -3.2935e-01,\n",
       "           -3.2935e-01, -3.2935e-01],\n",
       "          [-5.4115e-02, -3.0344e-02, -8.3332e-02,  ..., -3.2024e-01,\n",
       "           -3.2024e-01, -3.2400e-01],\n",
       "          ...,\n",
       "          [ 4.7268e-02,  9.4321e-02,  5.8962e-02,  ...,  2.6186e-03,\n",
       "            5.3010e-03,  1.2956e-04],\n",
       "          [ 3.0221e-02,  1.7081e-02,  1.4778e-03,  ..., -2.5465e-03,\n",
       "           -2.5465e-03, -2.5436e-03],\n",
       "          [ 6.4678e-03,  9.1464e-03,  1.4910e-03,  ..., -2.5494e-03,\n",
       "           -2.5494e-03, -2.5522e-03]]],\n",
       "\n",
       "\n",
       "        [[[-2.2216e-05, -2.2216e-05, -2.2216e-05,  ..., -2.2216e-05,\n",
       "           -2.2216e-05, -2.2216e-05],\n",
       "          [-2.2216e-05, -2.2216e-05, -2.2216e-05,  ..., -2.2216e-05,\n",
       "           -2.2216e-05, -2.2216e-05],\n",
       "          [-2.2216e-05, -2.2216e-05, -2.2216e-05,  ..., -2.2216e-05,\n",
       "           -2.2216e-05, -2.2216e-05],\n",
       "          ...,\n",
       "          [ 5.2850e-03,  5.2850e-03,  5.2850e-03,  ...,  3.5638e-03,\n",
       "            3.5638e-03,  6.6432e-03],\n",
       "          [ 5.2850e-03,  1.0066e-02,  1.1773e-02,  ...,  3.5638e-03,\n",
       "            3.5638e-03,  6.6432e-03],\n",
       "          [ 1.1773e-02,  1.0066e-02,  1.1761e-02,  ...,  3.5638e-03,\n",
       "            3.5638e-03,  6.6432e-03]],\n",
       "\n",
       "         [[ 4.2265e-05,  4.2265e-05,  4.2265e-05,  ...,  4.2265e-05,\n",
       "            4.2265e-05,  4.2265e-05],\n",
       "          [ 4.2265e-05,  4.2265e-05,  4.2265e-05,  ...,  4.2265e-05,\n",
       "            4.2265e-05,  4.2265e-05],\n",
       "          [ 4.2265e-05,  4.2265e-05,  4.2265e-05,  ...,  4.2265e-05,\n",
       "            4.2265e-05,  4.2265e-05],\n",
       "          ...,\n",
       "          [-2.3602e-02, -2.3602e-02, -2.3602e-02,  ..., -1.8995e-02,\n",
       "           -1.8995e-02, -1.7921e-02],\n",
       "          [-2.3602e-02, -2.7007e-02, -3.1505e-02,  ..., -1.8995e-02,\n",
       "           -1.8995e-02, -1.7921e-02],\n",
       "          [-3.1505e-02, -2.7007e-02, -3.1477e-02,  ..., -1.8995e-02,\n",
       "           -1.8995e-02, -1.7921e-02]]],\n",
       "\n",
       "\n",
       "        [[[ 1.5777e-02,  7.0811e-03,  2.4102e-03,  ...,  3.7821e-03,\n",
       "            5.5545e-03, -3.1157e-06],\n",
       "          [ 1.3646e-02,  4.4913e-03, -2.1749e-03,  ..., -3.3866e-06,\n",
       "            3.7821e-03,  3.7821e-03],\n",
       "          [ 2.1456e-03,  6.6960e-03, -4.5451e-03,  ...,  5.3169e-03,\n",
       "           -3.3866e-06,  5.7948e-03],\n",
       "          ...,\n",
       "          [ 1.5969e-02,  3.3171e-03,  5.0824e-03,  ...,  5.0824e-03,\n",
       "            8.5554e-03,  2.3509e-02],\n",
       "          [ 1.2184e-02,  8.1707e-03,  8.1707e-03,  ...,  1.1644e-02,\n",
       "            1.7037e-02,  1.5272e-02],\n",
       "          [ 1.1111e-02,  7.7911e-03,  8.5554e-03,  ...,  1.8884e-02,\n",
       "            8.1707e-03,  9.9360e-03]],\n",
       "\n",
       "         [[ 1.2906e-02,  2.3823e-02,  2.2247e-02,  ...,  1.3402e-03,\n",
       "           -3.9663e-03,  5.9605e-06],\n",
       "          [ 1.9190e-02,  1.5876e-02,  2.0648e-02,  ...,  6.4210e-06,\n",
       "            1.3402e-03,  1.3402e-03],\n",
       "          [ 7.9161e-03, -4.7555e-03,  1.2714e-02,  ..., -3.7967e-03,\n",
       "            6.4210e-06, -4.1378e-03],\n",
       "          ...,\n",
       "          [ 2.0656e-04,  1.1749e-03, -3.6294e-03,  ..., -3.6294e-03,\n",
       "           -2.4056e-03, -1.2929e-02],\n",
       "          [-1.1272e-03, -2.2945e-03, -2.2945e-03,  ..., -1.0707e-03,\n",
       "           -4.5966e-03,  2.0767e-04],\n",
       "          [-1.0150e-03, -2.1848e-03, -2.4056e-03,  ..., -9.6226e-03,\n",
       "           -2.2945e-03, -7.0988e-03]]]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lab_images[:,1:,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "color_transform = transforms.Compose([transforms.Resize((224,224)),\n",
    "                                transforms.ToTensor()])\n",
    "color_dataset = datasets.ImageFolder(\n",
    "  \"E:\\\\Drives-Linux-ubuntu-2020\\\\home\\\\aswin\\\\Documents\\\\Deep-Learning-Projects\\\\open_images_dataset\\\\dataset\\\\test\", \n",
    "  transform=color_transform)\n",
    "color_dataloader = enumerate(torch.utils.data.DataLoader(color_dataset, batch_size=batch_size, shuffle=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "i, (color_images, color_labels) = next(iter(color_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.imwrite(\"image.png\", cv2.cvtColor(color_images[0].permute(1, 2, 0).numpy()*255, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight\n",
      "conv1.bias\n",
      "conv2.weight\n",
      "conv2.bias\n",
      "conv3.weight\n",
      "conv3.bias\n",
      "conv4.weight\n",
      "conv4.bias\n",
      "conv5.weight\n",
      "conv5.bias\n",
      "conv6.weight\n",
      "conv6.bias\n",
      "conv7.weight\n",
      "conv7.bias\n",
      "conv8.weight\n",
      "conv8.bias\n",
      "conv9.weight\n",
      "conv9.bias\n",
      "conv_10.weight\n",
      "conv_10.bias\n",
      "conv_11.weight\n",
      "conv_11.bias\n",
      "conv_12.weight\n",
      "conv_12.bias\n",
      "conv_13.weight\n",
      "conv_13.bias\n",
      "model_inceptionv3.Conv2d_1a_3x3.conv.weight\n",
      "model_inceptionv3.Conv2d_1a_3x3.bn.weight\n",
      "model_inceptionv3.Conv2d_1a_3x3.bn.bias\n",
      "model_inceptionv3.Conv2d_2a_3x3.conv.weight\n",
      "model_inceptionv3.Conv2d_2a_3x3.bn.weight\n",
      "model_inceptionv3.Conv2d_2a_3x3.bn.bias\n",
      "model_inceptionv3.Conv2d_2b_3x3.conv.weight\n",
      "model_inceptionv3.Conv2d_2b_3x3.bn.weight\n",
      "model_inceptionv3.Conv2d_2b_3x3.bn.bias\n",
      "model_inceptionv3.Conv2d_3b_1x1.conv.weight\n",
      "model_inceptionv3.Conv2d_3b_1x1.bn.weight\n",
      "model_inceptionv3.Conv2d_3b_1x1.bn.bias\n",
      "model_inceptionv3.Conv2d_4a_3x3.conv.weight\n",
      "model_inceptionv3.Conv2d_4a_3x3.bn.weight\n",
      "model_inceptionv3.Conv2d_4a_3x3.bn.bias\n",
      "model_inceptionv3.Mixed_5b.branch1x1.conv.weight\n",
      "model_inceptionv3.Mixed_5b.branch1x1.bn.weight\n",
      "model_inceptionv3.Mixed_5b.branch1x1.bn.bias\n",
      "model_inceptionv3.Mixed_5b.branch5x5_1.conv.weight\n",
      "model_inceptionv3.Mixed_5b.branch5x5_1.bn.weight\n",
      "model_inceptionv3.Mixed_5b.branch5x5_1.bn.bias\n",
      "model_inceptionv3.Mixed_5b.branch5x5_2.conv.weight\n",
      "model_inceptionv3.Mixed_5b.branch5x5_2.bn.weight\n",
      "model_inceptionv3.Mixed_5b.branch5x5_2.bn.bias\n",
      "model_inceptionv3.Mixed_5b.branch3x3dbl_1.conv.weight\n",
      "model_inceptionv3.Mixed_5b.branch3x3dbl_1.bn.weight\n",
      "model_inceptionv3.Mixed_5b.branch3x3dbl_1.bn.bias\n",
      "model_inceptionv3.Mixed_5b.branch3x3dbl_2.conv.weight\n",
      "model_inceptionv3.Mixed_5b.branch3x3dbl_2.bn.weight\n",
      "model_inceptionv3.Mixed_5b.branch3x3dbl_2.bn.bias\n",
      "model_inceptionv3.Mixed_5b.branch3x3dbl_3.conv.weight\n",
      "model_inceptionv3.Mixed_5b.branch3x3dbl_3.bn.weight\n",
      "model_inceptionv3.Mixed_5b.branch3x3dbl_3.bn.bias\n",
      "model_inceptionv3.Mixed_5b.branch_pool.conv.weight\n",
      "model_inceptionv3.Mixed_5b.branch_pool.bn.weight\n",
      "model_inceptionv3.Mixed_5b.branch_pool.bn.bias\n",
      "model_inceptionv3.Mixed_5c.branch1x1.conv.weight\n",
      "model_inceptionv3.Mixed_5c.branch1x1.bn.weight\n",
      "model_inceptionv3.Mixed_5c.branch1x1.bn.bias\n",
      "model_inceptionv3.Mixed_5c.branch5x5_1.conv.weight\n",
      "model_inceptionv3.Mixed_5c.branch5x5_1.bn.weight\n",
      "model_inceptionv3.Mixed_5c.branch5x5_1.bn.bias\n",
      "model_inceptionv3.Mixed_5c.branch5x5_2.conv.weight\n",
      "model_inceptionv3.Mixed_5c.branch5x5_2.bn.weight\n",
      "model_inceptionv3.Mixed_5c.branch5x5_2.bn.bias\n",
      "model_inceptionv3.Mixed_5c.branch3x3dbl_1.conv.weight\n",
      "model_inceptionv3.Mixed_5c.branch3x3dbl_1.bn.weight\n",
      "model_inceptionv3.Mixed_5c.branch3x3dbl_1.bn.bias\n",
      "model_inceptionv3.Mixed_5c.branch3x3dbl_2.conv.weight\n",
      "model_inceptionv3.Mixed_5c.branch3x3dbl_2.bn.weight\n",
      "model_inceptionv3.Mixed_5c.branch3x3dbl_2.bn.bias\n",
      "model_inceptionv3.Mixed_5c.branch3x3dbl_3.conv.weight\n",
      "model_inceptionv3.Mixed_5c.branch3x3dbl_3.bn.weight\n",
      "model_inceptionv3.Mixed_5c.branch3x3dbl_3.bn.bias\n",
      "model_inceptionv3.Mixed_5c.branch_pool.conv.weight\n",
      "model_inceptionv3.Mixed_5c.branch_pool.bn.weight\n",
      "model_inceptionv3.Mixed_5c.branch_pool.bn.bias\n",
      "model_inceptionv3.Mixed_5d.branch1x1.conv.weight\n",
      "model_inceptionv3.Mixed_5d.branch1x1.bn.weight\n",
      "model_inceptionv3.Mixed_5d.branch1x1.bn.bias\n",
      "model_inceptionv3.Mixed_5d.branch5x5_1.conv.weight\n",
      "model_inceptionv3.Mixed_5d.branch5x5_1.bn.weight\n",
      "model_inceptionv3.Mixed_5d.branch5x5_1.bn.bias\n",
      "model_inceptionv3.Mixed_5d.branch5x5_2.conv.weight\n",
      "model_inceptionv3.Mixed_5d.branch5x5_2.bn.weight\n",
      "model_inceptionv3.Mixed_5d.branch5x5_2.bn.bias\n",
      "model_inceptionv3.Mixed_5d.branch3x3dbl_1.conv.weight\n",
      "model_inceptionv3.Mixed_5d.branch3x3dbl_1.bn.weight\n",
      "model_inceptionv3.Mixed_5d.branch3x3dbl_1.bn.bias\n",
      "model_inceptionv3.Mixed_5d.branch3x3dbl_2.conv.weight\n",
      "model_inceptionv3.Mixed_5d.branch3x3dbl_2.bn.weight\n",
      "model_inceptionv3.Mixed_5d.branch3x3dbl_2.bn.bias\n",
      "model_inceptionv3.Mixed_5d.branch3x3dbl_3.conv.weight\n",
      "model_inceptionv3.Mixed_5d.branch3x3dbl_3.bn.weight\n",
      "model_inceptionv3.Mixed_5d.branch3x3dbl_3.bn.bias\n",
      "model_inceptionv3.Mixed_5d.branch_pool.conv.weight\n",
      "model_inceptionv3.Mixed_5d.branch_pool.bn.weight\n",
      "model_inceptionv3.Mixed_5d.branch_pool.bn.bias\n",
      "model_inceptionv3.Mixed_6a.branch3x3.conv.weight\n",
      "model_inceptionv3.Mixed_6a.branch3x3.bn.weight\n",
      "model_inceptionv3.Mixed_6a.branch3x3.bn.bias\n",
      "model_inceptionv3.Mixed_6a.branch3x3dbl_1.conv.weight\n",
      "model_inceptionv3.Mixed_6a.branch3x3dbl_1.bn.weight\n",
      "model_inceptionv3.Mixed_6a.branch3x3dbl_1.bn.bias\n",
      "model_inceptionv3.Mixed_6a.branch3x3dbl_2.conv.weight\n",
      "model_inceptionv3.Mixed_6a.branch3x3dbl_2.bn.weight\n",
      "model_inceptionv3.Mixed_6a.branch3x3dbl_2.bn.bias\n",
      "model_inceptionv3.Mixed_6a.branch3x3dbl_3.conv.weight\n",
      "model_inceptionv3.Mixed_6a.branch3x3dbl_3.bn.weight\n",
      "model_inceptionv3.Mixed_6a.branch3x3dbl_3.bn.bias\n",
      "model_inceptionv3.Mixed_6b.branch1x1.conv.weight\n",
      "model_inceptionv3.Mixed_6b.branch1x1.bn.weight\n",
      "model_inceptionv3.Mixed_6b.branch1x1.bn.bias\n",
      "model_inceptionv3.Mixed_6b.branch7x7_1.conv.weight\n",
      "model_inceptionv3.Mixed_6b.branch7x7_1.bn.weight\n",
      "model_inceptionv3.Mixed_6b.branch7x7_1.bn.bias\n",
      "model_inceptionv3.Mixed_6b.branch7x7_2.conv.weight\n",
      "model_inceptionv3.Mixed_6b.branch7x7_2.bn.weight\n",
      "model_inceptionv3.Mixed_6b.branch7x7_2.bn.bias\n",
      "model_inceptionv3.Mixed_6b.branch7x7_3.conv.weight\n",
      "model_inceptionv3.Mixed_6b.branch7x7_3.bn.weight\n",
      "model_inceptionv3.Mixed_6b.branch7x7_3.bn.bias\n",
      "model_inceptionv3.Mixed_6b.branch7x7dbl_1.conv.weight\n",
      "model_inceptionv3.Mixed_6b.branch7x7dbl_1.bn.weight\n",
      "model_inceptionv3.Mixed_6b.branch7x7dbl_1.bn.bias\n",
      "model_inceptionv3.Mixed_6b.branch7x7dbl_2.conv.weight\n",
      "model_inceptionv3.Mixed_6b.branch7x7dbl_2.bn.weight\n",
      "model_inceptionv3.Mixed_6b.branch7x7dbl_2.bn.bias\n",
      "model_inceptionv3.Mixed_6b.branch7x7dbl_3.conv.weight\n",
      "model_inceptionv3.Mixed_6b.branch7x7dbl_3.bn.weight\n",
      "model_inceptionv3.Mixed_6b.branch7x7dbl_3.bn.bias\n",
      "model_inceptionv3.Mixed_6b.branch7x7dbl_4.conv.weight\n",
      "model_inceptionv3.Mixed_6b.branch7x7dbl_4.bn.weight\n",
      "model_inceptionv3.Mixed_6b.branch7x7dbl_4.bn.bias\n",
      "model_inceptionv3.Mixed_6b.branch7x7dbl_5.conv.weight\n",
      "model_inceptionv3.Mixed_6b.branch7x7dbl_5.bn.weight\n",
      "model_inceptionv3.Mixed_6b.branch7x7dbl_5.bn.bias\n",
      "model_inceptionv3.Mixed_6b.branch_pool.conv.weight\n",
      "model_inceptionv3.Mixed_6b.branch_pool.bn.weight\n",
      "model_inceptionv3.Mixed_6b.branch_pool.bn.bias\n",
      "model_inceptionv3.Mixed_6c.branch1x1.conv.weight\n",
      "model_inceptionv3.Mixed_6c.branch1x1.bn.weight\n",
      "model_inceptionv3.Mixed_6c.branch1x1.bn.bias\n",
      "model_inceptionv3.Mixed_6c.branch7x7_1.conv.weight\n",
      "model_inceptionv3.Mixed_6c.branch7x7_1.bn.weight\n",
      "model_inceptionv3.Mixed_6c.branch7x7_1.bn.bias\n",
      "model_inceptionv3.Mixed_6c.branch7x7_2.conv.weight\n",
      "model_inceptionv3.Mixed_6c.branch7x7_2.bn.weight\n",
      "model_inceptionv3.Mixed_6c.branch7x7_2.bn.bias\n",
      "model_inceptionv3.Mixed_6c.branch7x7_3.conv.weight\n",
      "model_inceptionv3.Mixed_6c.branch7x7_3.bn.weight\n",
      "model_inceptionv3.Mixed_6c.branch7x7_3.bn.bias\n",
      "model_inceptionv3.Mixed_6c.branch7x7dbl_1.conv.weight\n",
      "model_inceptionv3.Mixed_6c.branch7x7dbl_1.bn.weight\n",
      "model_inceptionv3.Mixed_6c.branch7x7dbl_1.bn.bias\n",
      "model_inceptionv3.Mixed_6c.branch7x7dbl_2.conv.weight\n",
      "model_inceptionv3.Mixed_6c.branch7x7dbl_2.bn.weight\n",
      "model_inceptionv3.Mixed_6c.branch7x7dbl_2.bn.bias\n",
      "model_inceptionv3.Mixed_6c.branch7x7dbl_3.conv.weight\n",
      "model_inceptionv3.Mixed_6c.branch7x7dbl_3.bn.weight\n",
      "model_inceptionv3.Mixed_6c.branch7x7dbl_3.bn.bias\n",
      "model_inceptionv3.Mixed_6c.branch7x7dbl_4.conv.weight\n",
      "model_inceptionv3.Mixed_6c.branch7x7dbl_4.bn.weight\n",
      "model_inceptionv3.Mixed_6c.branch7x7dbl_4.bn.bias\n",
      "model_inceptionv3.Mixed_6c.branch7x7dbl_5.conv.weight\n",
      "model_inceptionv3.Mixed_6c.branch7x7dbl_5.bn.weight\n",
      "model_inceptionv3.Mixed_6c.branch7x7dbl_5.bn.bias\n",
      "model_inceptionv3.Mixed_6c.branch_pool.conv.weight\n",
      "model_inceptionv3.Mixed_6c.branch_pool.bn.weight\n",
      "model_inceptionv3.Mixed_6c.branch_pool.bn.bias\n",
      "model_inceptionv3.Mixed_6d.branch1x1.conv.weight\n",
      "model_inceptionv3.Mixed_6d.branch1x1.bn.weight\n",
      "model_inceptionv3.Mixed_6d.branch1x1.bn.bias\n",
      "model_inceptionv3.Mixed_6d.branch7x7_1.conv.weight\n",
      "model_inceptionv3.Mixed_6d.branch7x7_1.bn.weight\n",
      "model_inceptionv3.Mixed_6d.branch7x7_1.bn.bias\n",
      "model_inceptionv3.Mixed_6d.branch7x7_2.conv.weight\n",
      "model_inceptionv3.Mixed_6d.branch7x7_2.bn.weight\n",
      "model_inceptionv3.Mixed_6d.branch7x7_2.bn.bias\n",
      "model_inceptionv3.Mixed_6d.branch7x7_3.conv.weight\n",
      "model_inceptionv3.Mixed_6d.branch7x7_3.bn.weight\n",
      "model_inceptionv3.Mixed_6d.branch7x7_3.bn.bias\n",
      "model_inceptionv3.Mixed_6d.branch7x7dbl_1.conv.weight\n",
      "model_inceptionv3.Mixed_6d.branch7x7dbl_1.bn.weight\n",
      "model_inceptionv3.Mixed_6d.branch7x7dbl_1.bn.bias\n",
      "model_inceptionv3.Mixed_6d.branch7x7dbl_2.conv.weight\n",
      "model_inceptionv3.Mixed_6d.branch7x7dbl_2.bn.weight\n",
      "model_inceptionv3.Mixed_6d.branch7x7dbl_2.bn.bias\n",
      "model_inceptionv3.Mixed_6d.branch7x7dbl_3.conv.weight\n",
      "model_inceptionv3.Mixed_6d.branch7x7dbl_3.bn.weight\n",
      "model_inceptionv3.Mixed_6d.branch7x7dbl_3.bn.bias\n",
      "model_inceptionv3.Mixed_6d.branch7x7dbl_4.conv.weight\n",
      "model_inceptionv3.Mixed_6d.branch7x7dbl_4.bn.weight\n",
      "model_inceptionv3.Mixed_6d.branch7x7dbl_4.bn.bias\n",
      "model_inceptionv3.Mixed_6d.branch7x7dbl_5.conv.weight\n",
      "model_inceptionv3.Mixed_6d.branch7x7dbl_5.bn.weight\n",
      "model_inceptionv3.Mixed_6d.branch7x7dbl_5.bn.bias\n",
      "model_inceptionv3.Mixed_6d.branch_pool.conv.weight\n",
      "model_inceptionv3.Mixed_6d.branch_pool.bn.weight\n",
      "model_inceptionv3.Mixed_6d.branch_pool.bn.bias\n",
      "model_inceptionv3.Mixed_6e.branch1x1.conv.weight\n",
      "model_inceptionv3.Mixed_6e.branch1x1.bn.weight\n",
      "model_inceptionv3.Mixed_6e.branch1x1.bn.bias\n",
      "model_inceptionv3.Mixed_6e.branch7x7_1.conv.weight\n",
      "model_inceptionv3.Mixed_6e.branch7x7_1.bn.weight\n",
      "model_inceptionv3.Mixed_6e.branch7x7_1.bn.bias\n",
      "model_inceptionv3.Mixed_6e.branch7x7_2.conv.weight\n",
      "model_inceptionv3.Mixed_6e.branch7x7_2.bn.weight\n",
      "model_inceptionv3.Mixed_6e.branch7x7_2.bn.bias\n",
      "model_inceptionv3.Mixed_6e.branch7x7_3.conv.weight\n",
      "model_inceptionv3.Mixed_6e.branch7x7_3.bn.weight\n",
      "model_inceptionv3.Mixed_6e.branch7x7_3.bn.bias\n",
      "model_inceptionv3.Mixed_6e.branch7x7dbl_1.conv.weight\n",
      "model_inceptionv3.Mixed_6e.branch7x7dbl_1.bn.weight\n",
      "model_inceptionv3.Mixed_6e.branch7x7dbl_1.bn.bias\n",
      "model_inceptionv3.Mixed_6e.branch7x7dbl_2.conv.weight\n",
      "model_inceptionv3.Mixed_6e.branch7x7dbl_2.bn.weight\n",
      "model_inceptionv3.Mixed_6e.branch7x7dbl_2.bn.bias\n",
      "model_inceptionv3.Mixed_6e.branch7x7dbl_3.conv.weight\n",
      "model_inceptionv3.Mixed_6e.branch7x7dbl_3.bn.weight\n",
      "model_inceptionv3.Mixed_6e.branch7x7dbl_3.bn.bias\n",
      "model_inceptionv3.Mixed_6e.branch7x7dbl_4.conv.weight\n",
      "model_inceptionv3.Mixed_6e.branch7x7dbl_4.bn.weight\n",
      "model_inceptionv3.Mixed_6e.branch7x7dbl_4.bn.bias\n",
      "model_inceptionv3.Mixed_6e.branch7x7dbl_5.conv.weight\n",
      "model_inceptionv3.Mixed_6e.branch7x7dbl_5.bn.weight\n",
      "model_inceptionv3.Mixed_6e.branch7x7dbl_5.bn.bias\n",
      "model_inceptionv3.Mixed_6e.branch_pool.conv.weight\n",
      "model_inceptionv3.Mixed_6e.branch_pool.bn.weight\n",
      "model_inceptionv3.Mixed_6e.branch_pool.bn.bias\n",
      "model_inceptionv3.Mixed_7a.branch3x3_1.conv.weight\n",
      "model_inceptionv3.Mixed_7a.branch3x3_1.bn.weight\n",
      "model_inceptionv3.Mixed_7a.branch3x3_1.bn.bias\n",
      "model_inceptionv3.Mixed_7a.branch3x3_2.conv.weight\n",
      "model_inceptionv3.Mixed_7a.branch3x3_2.bn.weight\n",
      "model_inceptionv3.Mixed_7a.branch3x3_2.bn.bias\n",
      "model_inceptionv3.Mixed_7a.branch7x7x3_1.conv.weight\n",
      "model_inceptionv3.Mixed_7a.branch7x7x3_1.bn.weight\n",
      "model_inceptionv3.Mixed_7a.branch7x7x3_1.bn.bias\n",
      "model_inceptionv3.Mixed_7a.branch7x7x3_2.conv.weight\n",
      "model_inceptionv3.Mixed_7a.branch7x7x3_2.bn.weight\n",
      "model_inceptionv3.Mixed_7a.branch7x7x3_2.bn.bias\n",
      "model_inceptionv3.Mixed_7a.branch7x7x3_3.conv.weight\n",
      "model_inceptionv3.Mixed_7a.branch7x7x3_3.bn.weight\n",
      "model_inceptionv3.Mixed_7a.branch7x7x3_3.bn.bias\n",
      "model_inceptionv3.Mixed_7a.branch7x7x3_4.conv.weight\n",
      "model_inceptionv3.Mixed_7a.branch7x7x3_4.bn.weight\n",
      "model_inceptionv3.Mixed_7a.branch7x7x3_4.bn.bias\n",
      "model_inceptionv3.Mixed_7b.branch1x1.conv.weight\n",
      "model_inceptionv3.Mixed_7b.branch1x1.bn.weight\n",
      "model_inceptionv3.Mixed_7b.branch1x1.bn.bias\n",
      "model_inceptionv3.Mixed_7b.branch3x3_1.conv.weight\n",
      "model_inceptionv3.Mixed_7b.branch3x3_1.bn.weight\n",
      "model_inceptionv3.Mixed_7b.branch3x3_1.bn.bias\n",
      "model_inceptionv3.Mixed_7b.branch3x3_2a.conv.weight\n",
      "model_inceptionv3.Mixed_7b.branch3x3_2a.bn.weight\n",
      "model_inceptionv3.Mixed_7b.branch3x3_2a.bn.bias\n",
      "model_inceptionv3.Mixed_7b.branch3x3_2b.conv.weight\n",
      "model_inceptionv3.Mixed_7b.branch3x3_2b.bn.weight\n",
      "model_inceptionv3.Mixed_7b.branch3x3_2b.bn.bias\n",
      "model_inceptionv3.Mixed_7b.branch3x3dbl_1.conv.weight\n",
      "model_inceptionv3.Mixed_7b.branch3x3dbl_1.bn.weight\n",
      "model_inceptionv3.Mixed_7b.branch3x3dbl_1.bn.bias\n",
      "model_inceptionv3.Mixed_7b.branch3x3dbl_2.conv.weight\n",
      "model_inceptionv3.Mixed_7b.branch3x3dbl_2.bn.weight\n",
      "model_inceptionv3.Mixed_7b.branch3x3dbl_2.bn.bias\n",
      "model_inceptionv3.Mixed_7b.branch3x3dbl_3a.conv.weight\n",
      "model_inceptionv3.Mixed_7b.branch3x3dbl_3a.bn.weight\n",
      "model_inceptionv3.Mixed_7b.branch3x3dbl_3a.bn.bias\n",
      "model_inceptionv3.Mixed_7b.branch3x3dbl_3b.conv.weight\n",
      "model_inceptionv3.Mixed_7b.branch3x3dbl_3b.bn.weight\n",
      "model_inceptionv3.Mixed_7b.branch3x3dbl_3b.bn.bias\n",
      "model_inceptionv3.Mixed_7b.branch_pool.conv.weight\n",
      "model_inceptionv3.Mixed_7b.branch_pool.bn.weight\n",
      "model_inceptionv3.Mixed_7b.branch_pool.bn.bias\n",
      "model_inceptionv3.Mixed_7c.branch1x1.conv.weight\n",
      "model_inceptionv3.Mixed_7c.branch1x1.bn.weight\n",
      "model_inceptionv3.Mixed_7c.branch1x1.bn.bias\n",
      "model_inceptionv3.Mixed_7c.branch3x3_1.conv.weight\n",
      "model_inceptionv3.Mixed_7c.branch3x3_1.bn.weight\n",
      "model_inceptionv3.Mixed_7c.branch3x3_1.bn.bias\n",
      "model_inceptionv3.Mixed_7c.branch3x3_2a.conv.weight\n",
      "model_inceptionv3.Mixed_7c.branch3x3_2a.bn.weight\n",
      "model_inceptionv3.Mixed_7c.branch3x3_2a.bn.bias\n",
      "model_inceptionv3.Mixed_7c.branch3x3_2b.conv.weight\n",
      "model_inceptionv3.Mixed_7c.branch3x3_2b.bn.weight\n",
      "model_inceptionv3.Mixed_7c.branch3x3_2b.bn.bias\n",
      "model_inceptionv3.Mixed_7c.branch3x3dbl_1.conv.weight\n",
      "model_inceptionv3.Mixed_7c.branch3x3dbl_1.bn.weight\n",
      "model_inceptionv3.Mixed_7c.branch3x3dbl_1.bn.bias\n",
      "model_inceptionv3.Mixed_7c.branch3x3dbl_2.conv.weight\n",
      "model_inceptionv3.Mixed_7c.branch3x3dbl_2.bn.weight\n",
      "model_inceptionv3.Mixed_7c.branch3x3dbl_2.bn.bias\n",
      "model_inceptionv3.Mixed_7c.branch3x3dbl_3a.conv.weight\n",
      "model_inceptionv3.Mixed_7c.branch3x3dbl_3a.bn.weight\n",
      "model_inceptionv3.Mixed_7c.branch3x3dbl_3a.bn.bias\n",
      "model_inceptionv3.Mixed_7c.branch3x3dbl_3b.conv.weight\n",
      "model_inceptionv3.Mixed_7c.branch3x3dbl_3b.bn.weight\n",
      "model_inceptionv3.Mixed_7c.branch3x3dbl_3b.bn.bias\n",
      "model_inceptionv3.Mixed_7c.branch_pool.conv.weight\n",
      "model_inceptionv3.Mixed_7c.branch_pool.bn.weight\n",
      "model_inceptionv3.Mixed_7c.branch_pool.bn.bias\n",
      "model_inceptionv3.fc.weight\n",
      "model_inceptionv3.fc.bias\n",
      "linear1.weight\n",
      "linear1.bias\n",
      "linear2.weight\n",
      "linear2.bias\n",
      "conv_15.weight\n",
      "conv_15.bias\n",
      "conv_16.weight\n",
      "conv_16.bias\n",
      "conv_17.weight\n",
      "conv_17.bias\n",
      "conv_18.weight\n",
      "conv_18.bias\n",
      "conv_19.weight\n",
      "conv_19.bias\n",
      "conv_20.weight\n",
      "conv_20.bias\n",
      "conv_21.weight\n",
      "conv_21.bias\n",
      "conv_22.weight\n",
      "conv_22.bias\n"
     ]
    }
   ],
   "source": [
    "net = ImageColorNet(32)\n",
    "for i, param in net.named_parameters():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=False)\n",
    "images, labels = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list1=[1,2,3,4,5]\n",
    "list1=iter(list1)\n",
    "# It will return first value of iterator i.e. '1'\n",
    "print(next(list1))\n",
    "# It will return second value of iterator i.e. '2'\n",
    "print(next(list1))\n",
    "# It will return third value of iterator i.e. '3'\n",
    "print(next(list1))\n",
    "# It will return fourth value of iterator i.e. '4'\n",
    "print(next(list1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
